<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd" xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/">
    <dc:contributor>GUERRERO CAMPO, JOSE JESUS</dc:contributor>
    <dc:contributor>LOPEZ NICOLAS, GONZALO</dc:contributor>
    <dc:creator>Pérez Yus, Alejandro</dc:creator>
    <dc:date>2018</dc:date>
    <dc:description>La visión por computador es un campo de investigación en continua expansión, que tiene como principal objetivo desarrollar algoritmos para extraer información del entorno a partir de imágenes. Las aplicaciones de este campo son innumerables, y continuos avances muestran un prometedor futuro por delante. En la actualidad, la visión por computador se encuentra presente en muchas áreas, entre las que pueden incluirse, por ejemplo, la conducción autónoma, el control de drones o la robótica. En particular, nuestra motivación está dentro del contexto de la asistencia personal, en donde las técnicas de visión por computador se desarrollan para estar al servicio de las personas, y especialmente de personas con discapacidades. En esta tesis, nos centramos en el problema de desarrollar un asistente basado en sistemas de visión artificial que permita a las personas con discapacidad visual desplazarse de manera segura y eficiente. Existen dispositivos ampliamente extendidos para la ayuda de ciegos, como el bastón blanco o el perro lazarillo. Si bien este tipo de dispositivos son de utilidad en situaciones simples y de corto alcance, creemos que ha llegado el momento de utilizar los avances tecnológicos para mejorar la capacidad y autonomía del usuario al proporcionarle información adicional.&lt;br /&gt;La aparición reciente en el mercado de cámaras RGB-D de bajo coste, así como sus posibilidades de miniaturización, demuestran que el desarrollo de sistemas portables con dicha tecnología es más que plausible en un futuro próximo. En esta tesis el uso de cámaras RGB-D es un elemento central, ya que permiten capturar simultáneamente información tridimensional junto con el color, siendo especialmente útiles para detectar formas y obstáculos. Para superar algunas limitaciones de este tipo de cámaras, se ha investigado su uso combinado con cámaras omnidireccionales, creando nuevos sistemas que permitan capturar más información del entorno desde un único punto de vista.&lt;br /&gt;Un objetivo general de esta tesis ha sido desarrollar diversos métodos para obtener información relevante de la escena que pueda ayudar en la navegación humana. De la amplia gama de posibles tipos de información que se pueden obtener del entorno, nos centramos en la extracción de información estructural representativa de cualquier entorno construido por el hombre. Particularmente, hemos desarrollado un sistema capaz de encontrar el suelo y la orientación de la escena, así como los posibles obstáculos presentes en el entorno. Este sistema de navegación, que está basado en el uso de la percepción de profundidad de la escena, ha sido desarrollado dando como resultado la contribución de un algoritmo de detección, modelado y recorrido de escaleras. El métodopropuesto es capaz de detectar escaleras ascendentes y descendentes, obtener su orientación y dimensiones, así como re-localizar continuamente al sujeto durante su recorrido utilizando un algoritmo de odometría visual que funciona en paralelo.&lt;br /&gt;Con el objetivo de proporcionar un mejor entendimiento visual de la escena, proponemos estimar la distribución estructural de la habitación, lo cual podría ayudar en tareas como navegación, reconocimiento de escenas o detección de objetos. Sin embargo, una de las principales limitaciones de las cámaras RGB-D es su reducido campo de vista. Para superar este inconveniente, se presenta la siguiente contribución que consiste en el desarrollo y calibración de un novedoso sistema de visión que permite extender el campo de vista mediante cámaras omnidireccionales. Concretamente, se ha desarrollado un nuevo sistema híbrido con lente de ojo de pez y cámara de profundidad, así como el necesario método de calibración del sistema. Además, para extender el método desarrollado a otros sistemas, se ha elaborado un segundo método de calibración basado&lt;br /&gt;en observaciones de líneas que nos permite calibrar múltiples combinaciones de cámaras, sin requerimiento de solapamiento de campos de vista y sin necesidad de construir ningún patrón de calibración. El sistema híbrido de cámaras desarrollado permite vislumbrar nuevas posibilidades, que motivan la siguiente contribución: el diseño de un método de estimación de la estructura del entorno que permite obtener reconstrucciones 3D a escala de la escena. Este método se fundamenta en el uso del amplio campo de vista del ojo de pez y de la percepción 3D de la cámara de profundidad.&lt;br /&gt;Si bien la extracción de información relevante del entorno es un problema relevante, también lo es la comunicación de la información obtenida al usuario. Éste es un problema complejo que no es tratado en la mayoría de sistemas de asistencia basados en visión por computador de la literatura. Dados los nuevos avances en visión prostética, consideramos viable el contribuir en este área con las técnicas de visión por computador desarrolladas en esta tesis. Actualmente, pacientes con prótesis visuales son capaces de ver una distribución de puntos de luz, llamados fosfenos. Sin embargo, actualmente el conjunto de fosfenos tienen resolución espacial y rango dinámico limitados. Nuestra siguiente contribución es el desarrollo de algoritmos de codificación de la información percibida en patrones de fosfenos, mediante una representación icónica que hace posible entender la escena para su navegación. Partiendo de la información de profundidad proporcionada por el sensor, el método desarrollado permite al usuario percibir a través de la codificación en fosfenos algo tan necesario como es la sensación de movimiento y profundidad del entorno, siendo la clave en técnicas de representación icónica propuestas.&lt;br /&gt;</dc:description>
    <dc:description>Computer vision is an impressively expanding field of research that aims to develop algorithms to extract information of the environment using images. The applications of such field are endless, and continuous advances show a bright future ahead. Nowadays computer vision is applied to many areas, such as autonomous driving, drones or robotics. In particular, our motivation lies within the context of human assistance, where computer vision techniques are developed to be of service of people, especially people with disabilities. In this thesis, we particularly focus on the problem of developing a camera-based assistant that allows visually impaired people to move safe and efficiently. Popular low-tech aids for the blind, such as the white cane, work well for simple, short-range situations, but we believe technological advances can enhance the overall experience by providing useful information. Particularly, the advent a few years ago of consumer RGB-D cameras and its current miniaturization possibilities paves the way for a wearable system with such type of camera quite affordable and plausible in the near future. We use RGB-D cameras as a central element of our system, since they provide three-dimensional information alongside color, making them especially useful to detect shapes and obstacles. Additionally, we have also explored their combination with omnidirectional cameras, to create new powerful systems able to capture more information of the environment at once. A main goal of this thesis is to develop a variety of methods to recover relevant information of the scene that would help in human navigation. Since this problem is too general, we focused on addressing the extraction of common structural information that can be found in any man-made environment. Particularly, we developed a system able to find the floor and the orientation of the scene, in which obstacles along the way could be detected using depth perception. We extended this navigational system leading to our first major contribution, which is the elaboration of a stair detection, modeling and traversal algorithm. This proposed method is able to detect ascending and descending staircases, obtain their orientation and dimensions, and continuously re-localize the subject during the traversal using a visual odometry algorithm running in parallel. Additionally, in order to enable a better understanding of the scene, we propose to estimate the layout of the room, which could help in tasks such as navigation, scene recognition or object detection. However, one of the main limitations of the RGB-D cameras is their narrow field of view. In order to overcome this limitation, our next contribution was to develop and calibrate novel camera systems to extend the field of view by means of omnidirectional cameras. In particular, we developed a fisheye and depth hybrid camera system and the corresponding method of calibration for these type of systems. Moreover, to extend the possibilities to other systems, we developed a second calibration method based on line observations able to calibrate multiple camera combinations, without overlapping field of view requirements and without needing to build a calibration pattern. With our hybrid camera system we open new possibilities such as the design of a layout estimation method, able to obtain full-scaled 3D reconstructions of the scene, benefiting from the wide field of view of the fisheye and the data from the depth camera, which is also a relevant contribution. The communication of the perceived information to the user is a complex problem not directly treated in many of the assistive computer vision systems of the literature. Given the new advances in prosthetic vision, we investigated the application of the developed computer vision techniques to this area. Patients with visual prosthesis are able to see an array of light dots, called phosphenes. However, nowadays phosphene arrays have limited spatial resolution and dynamic range. Our next major contribution was the challenging task of codifying the information extracted to phosphene patterns, with an iconic representation that makes possible to understand the scene with the limitations given. In particular, we use the depth camera to extract free walking space in the scene, and using an iconic-based approach allows to provide comfortable and informative visual cues that other existing methods usually lack.&lt;br /&gt;</dc:description>
    <dc:identifier>http://zaguan.unizar.es/record/75364</dc:identifier>
    <dc:language>eng</dc:language>
    <dc:publisher>Universidad de Zaragoza; Departamento de Informática e Ingeniería de Sistemas</dc:publisher>
    <dc:relation>http://zaguan.unizar.es/record/75364/files/TESIS-2018-065.pdf</dc:relation>
    <dc:rights>https://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
    <dc:subject>robotica</dc:subject>
    <dc:title>Scene structure recovery from omnidirectional and depth cameras for assistive computer vision</dc:title>
    <dc:type>TESIS</dc:type>
</oai_dc:dc>
