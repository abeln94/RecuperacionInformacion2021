<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd" xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/">
    <dc:contributor>Lleida Solano, Eduardo</dc:contributor>
    <dc:creator>Almazán Galisteo, Eduardo</dc:creator>
    <dc:date>2018</dc:date>
    <dc:description>La gran cantidad de herramientas de búsqueda de documentos multimedia en bases de datos y la dificultad de la indexación de este tipo de contenido, hacen necesaria la existencia de sistemas que permitan agilizar el procesado de gran cantidad de datos para su correcto almacenamiento y uso. El sistema de descripción automática de video propuesto en este TFM facilita esta tarea. Partiendo de los sistemas ya existentes, basados en indexación descriptiva y añadiendo una capa más que permite combinar la información obtenida de los frames gracias a los descriptores utilizados. De esta forma, se atribuye al documento datos de mayor nivel sobre la significación, realizando una combinación entre la indexación semántica y descriptiva. Combinando conceptos de procesado de imagen, redes neuronales y procesado de lenguaje neuronal, se ha generado un sistema dividido en tres bloques diferentes. Estos se encargan de la detección de los frames principales a partir de descriptores estadísticos, la descripción de estos a partir de un modelo de captioning (descripción de imagen) basado en redes neuronales, y el procesado de las descripciones mediante el análisis de sus embeddings y el algoritmo LSA (Latent Semantic Analysis). Una vez desarrollado el sistema, se han explorado diferentes conjuntos de parámetros, con el objetivo de buscar el mejor ajuste posible. Por un lado, se han alcanzado valores de recall del 85.9% y 83.13% de precisión para el bloque de selección. Por otro, tras la comparación de diferentes modelos de redes neuronales, se ha obtenido una mejora del 38.76% respecto a los modelos más básicos mediante una red basada en modelos de atención y ajustada mediante SCST (Self Critical Sequence Training). Tras esto, se ajustaron los parámetros correspondientes al generador de resúmenes realizando un estudio de los resultados, ya que no existen bases de datos ni métricas con las que compararlos para obtener una cuantificación objetiva del error obtenido. Finalmente, se analizaron los resúmenes obtenidos para diferentes videos, observando un buen rendimiento general del sistema y destacando la gran variabilidad en el ajuste, provocando que no exista un conjunto de parámetros que permita obtener el mejor rendimiento de forma generalizada, así como la falta de conexión observada entre las frases del resumen debido al único uso del contenido descriptivo de los frames.</dc:description>
    <dc:identifier>http://zaguan.unizar.es/record/76229</dc:identifier>
    <dc:language>spa</dc:language>
    <dc:publisher>Universidad de Zaragoza; Departamento de Ingeniería Electrónica y Comunicaciones; Área de Teoría de la Señal y Comunicaciones</dc:publisher>
    <dc:relation>http://zaguan.unizar.es/record/76229/files/TAZ-TFM-2018-866.pdf</dc:relation>
    <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
    <dc:subject>Máster Universitario en Ingeniería de Telecomunicación</dc:subject>
    <dc:title>Sistema de descripción automática de vídeo para la generación de resúmenes audiovisuales</dc:title>
    <dc:type>TAZ-TFM</dc:type>
    <dc:title xml:lang="en">Automatic video description system for the generation of audiovisual summaries</dc:title>
</oai_dc:dc>
