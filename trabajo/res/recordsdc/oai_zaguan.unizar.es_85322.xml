<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd" xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/">
    <dc:contributor>Bermúdez Cameo, Jesús</dc:contributor>
    <dc:contributor>Guerrero Campo, José Jesús</dc:contributor>
    <dc:creator>Santos Villafranca, María</dc:creator>
    <dc:date>2019</dc:date>
    <dc:description>El sentido del que más dependen las personas es la visión, la mayor parte de la información recibida es a través de éste sentido. Es muy importante en la vida cotidiana, se usa constantemente hasta en las tareas más sencillas, aquellas como reconocer objetos, personas, etc. Sin embargo, algunas patologías o enfermedades degenerativas pueden causar la ceguera total o parcial. Como alternativa para paliar los efectos de la ceguera, existen diferentes tipos de prótesis visuales que se pueden situar en la retina, la corteza visual o el nervio óptico en función del problema que cause la ceguera. Una de las posibilidades es que estas prótesis tengan una microcámara que capta la información visual que posteriormente es convertida a estimulaciones eléctricas lo que permite ver puntos de luz denominados fosfenos. Desgraciadamente, el campo de visión actual de estos dispositivos es en torno a los 20º por lo que se están investigando diferentes representaciones de mapas de fosfenos para mejorar la interacción de los pacientes con el entorno. En este proyecto se ha desarrollado un simulador de visión fosfénica que permite, mediante las gafas de realidad virtual Oculus Rift DK2, mostrar un entorno virtual de 360º en tiempo real a partir de imágenes panorámicas. Este simulador permite introducir dos tipos de representaciones de mapas de fosfenos, el método Downsampling y el método SIE-OMS. El primer método reduce la resolución de color y espacial de la imagen panorámica y la transforma a un mapa de fosfenos mientras que el segundo realiza una extracción de objetos mediante un algoritmo de aprendizaje automático y lo combina con el layout de la imagen. El código se ha implementado en leguaje C++, con la propia interfaz de programación de aplicaciones (API) de Oculus y las librerías OpenCV para el manejo de imágenes. Ha sido necesario realizar un cursillo previo para aprender a manejar las librerías. Los resultados obtenidos han sido bastante satisfactorios cumpliendo todos los objetivos planteados. El objetivo principal era crear el simulador de prótesis visual que pueda ser empleado para el avance de la investigación de este campo.&lt;br /&gt;</dc:description>
    <dc:identifier>http://zaguan.unizar.es/record/85322</dc:identifier>
    <dc:language>spa</dc:language>
    <dc:publisher>Universidad de Zaragoza; Departamento de Informática e Ingeniería de Sistemas; Área de Ingeniería de Sistemas y Automática</dc:publisher>
    <dc:relation>http://zaguan.unizar.es/record/85322/files/TAZ-TFG-2019-2265.pdf</dc:relation>
    <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
    <dc:subject>Graduado en Ingeniería de Tecnologías Industriales</dc:subject>
    <dc:title>Simulador de prótesis visual en entornos 360º con gafas de realidad virtual</dc:title>
    <dc:type>TAZ-TFG</dc:type>
    <dc:title xml:lang="en">Visual prosthesis simulator in 360º environments with virtual reality goggles</dc:title>
</oai_dc:dc>
