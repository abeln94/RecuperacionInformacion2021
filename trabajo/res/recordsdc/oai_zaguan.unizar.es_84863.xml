<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd" xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/">
    <dc:contributor>Castellanos Gómez, José Ángel</dc:contributor>
    <dc:creator>Placed Perales, Julio Alberto</dc:creator>
    <dc:date>2019</dc:date>
    <dc:description>El SLAM (Simultanous Localisation and Mapping) activo hace referencia al problema de controlar el movimiento de un robot que está realizando SLAM, de forma que se minimice la incertidumbre del mapa creado y de su localización. Tradicionalmente ha sido resuelto mediante filtros u otras aproximaciones que involucran procesos de decisión de Markov o algoritmos de aprendizaje por refuerzo. En éstos, es necesario (i) identificar las posibles acciones, (ii) calcular el valor futuro esperado de cada una de ellas (e.g. mediante funciones de utilidad) y (iii) ejecutar la acción óptima. En este Trabajo Fin de Máster se analiza la resolución del problema mediante redes neuronales profundas, un campo de gran auge en la actualidad donde el aprendizaje por excelencia es el supervisado, que atrae la mayoría de investigaciones y aplicaciones de la literatura. La naturaleza del problema abordado, sin embargo, hace necesario el uso de otra forma de aprendizaje automático: el aprendizaje por refuerzo profundo. Se ha analizado el potencial y las limitaciones de este marco de trabajo, empleado normalmente en entornos de simulación sencillos, donde la diferencia entre exploración y navegación y el problema de generalización (clave en el SLAM activo, puesto que la información a priori del entorno es nula) son habitualmente obviados. Se han implementado distintas aproximaciones de aprendizaje por refuerzo y refuerzo profundo basadas en Q-learning sobre el entorno de simulación Gazebo. Ambos aprendizajes y su capacidad de generalización a escenarios desconocidos se estudian en profundidad, consiguiendo que agentes entrenados naveguen por entornos totalmente desconocidos. Además, se propone la inclusión de una métrica de la matriz de covarianza en la función de recompensa, consiguiendo una reducción de entropía paulatina durante la exploración y favoreciendo acciones mucho más óptimas en términos de reducción de la in- certidumbre.&lt;br /&gt;</dc:description>
    <dc:identifier>http://zaguan.unizar.es/record/84863</dc:identifier>
    <dc:language>spa</dc:language>
    <dc:publisher>Universidad de Zaragoza; Departamento de Informática e Ingeniería de Sistemas; Área de Ingeniería de Sistemas y Automática</dc:publisher>
    <dc:relation>http://zaguan.unizar.es/record/84863/files/TAZ-TFM-2019-255.pdf</dc:relation>
    <dc:relation>http://zaguan.unizar.es/record/84863/files/TAZ-TFM-2019-255_ANE.pdf</dc:relation>
    <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
    <dc:subject>Máster Universitario en Ingeniería Industrial</dc:subject>
    <dc:title>Estrategias de Deep Learning en SLAM Activo</dc:title>
    <dc:type>TAZ-TFM</dc:type>
    <dc:title xml:lang="en">Deep Learning Strategies in Active SLAM</dc:title>
</oai_dc:dc>
